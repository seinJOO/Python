{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://example.com/html/b.html\n",
      "https://example.com/html/sub/c.html\n",
      "https://example.com/index.html\n",
      "https://example.com/img/home.png\n",
      "https://example.com/css/home.css\n",
      "https://example.com/index.html\n",
      "http://otherExample.com/wiki\n",
      "https://otherExample.com/test\n"
     ]
    }
   ],
   "source": [
    "# 링크에 있는 내용을 한꺼번에 처리하기 (크롤링 & 스크래핑)\n",
    "\n",
    "# join(base, \"상대경로\") : 재귀적으로 탐색하기 위한 함수\n",
    "# 경로주소 처리(경로를 바꿔줌), 문자열로 반환\n",
    "\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "base = \"https://example.com/html/a.html\"\n",
    "print(urljoin(base, \"b.html\"))          # 경로 이동 - https://example.com/html/b.html\n",
    "print(urljoin(base, \"sub/c.html\"))      # sub/ = ../\n",
    "print(urljoin(base, \"../index.html\"))   # https://example.com/index.html\n",
    "print(urljoin(base, \"../img/home.png\")) # https://example.com/img/home.png\n",
    "print(urljoin(base, \"../css/home.css\")) # https://example.com/css/home.css\n",
    "print(urljoin(base, \"/index.html\"))     # https://example.com/index.html\n",
    "print(urljoin(base, \"http://otherExample.com/wiki\"))  # http://로 덮어쓰기 - http://otherExample.com/wiki\n",
    "print(urljoin(base, \"//otherExample.com/test\"))  # //로 덮어쓰기 - https://otherExample.com/test\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 처리 순서\n",
    "# 1. HTML 분석\n",
    "# 2. 링크 추출\n",
    "# 3. 각 링크 대상에 대해 다음 동작을 구동\n",
    "    # 3-1. 다운로드 파일인 경우 다운로드하기 (이미지파일, css파일 등..)\n",
    "    # 3-2. 파일이 HTML이라면 1번으로 돌아가서 다시 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 페이지를 한번에 다운받는 프로그램 만들기(html..)\n",
    "\n",
    "# 모듈\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import *\n",
    "from urllib.parse import *\n",
    "from os import makedirs\n",
    "import os.path, time, re\n",
    "\n",
    "# 처리한 내용이 파일인지 아닌지 확인을 위한 변수\n",
    "proc_files = {}\n",
    "\n",
    "# HTML 내부에 있는 링크를 추출하는 함수\n",
    "def enum_links(html, base) :\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "    links = soup.select(\"link[rel='stylesheet']\")   # css 불러오기\n",
    "    links += soup.select(\"a[href]\")     # href 속성을 가지고 있는 a태그 수집\n",
    "    \n",
    "    result = []    \n",
    "    # href 속성을 추출하고 링크를 절대경로로 변환하여 저장\n",
    "    for a in links :\n",
    "        href = a.attrs['href']      # a태그의 href 속성값만 추출\n",
    "        url = urljoin(base, href)   # base url을 a태그의 href 경로로 변경\n",
    "        result.append(url)\n",
    "    return result\n",
    "\n",
    "# 파일을 다운로드하고 저장하는 함수\n",
    "def download_file(url) :\n",
    "    o = urlparse(url)\n",
    "    savepath = \"./\"+ o.netloc + o.path\n",
    "    if re.search(r\"/$\", savepath) :     # savepath의 마지막이 '/'으로 끝나면 = 디렉토리라면\n",
    "        savepath += \"index.html\"        # 링크의 기본문서로 처리하기\n",
    "    savedir = os.path.dirname(savepath) # savepath의 디렉토리 정보만 빼오기\n",
    "    \n",
    "    # 모두 다운로드 되었는지 여부 확인\n",
    "    if os.path.exists(savepath) : return savepath       # 해당 이름의 파일이 존재한다면 리턴\n",
    "    # 다운로드가 되지 않았다면, 다운로드 받을 디렉터리 생성\n",
    "    if not os.path.exists(savedir) :\n",
    "        print(\"mkdir =\", savedir)\n",
    "        makedirs(savedir)\n",
    "    \n",
    "    # 파일 다운받기\n",
    "    try :\n",
    "        print(\"download =\", url)\n",
    "        urlretrieve(url, savepath)\n",
    "        time.sleep(3)   # 다운받는 시간 동안 프로그램 일시정지\n",
    "        return savepath\n",
    "    except :\n",
    "        print(\"다운로드 실패!\",url)\n",
    "        return None\n",
    "    \n",
    "# 다운받은 파일이 HTML인지 분석\n",
    "def analyze_html(url, root_url) :\n",
    "    savepath = download_file(url)\n",
    "    if savepath is None : return\n",
    "    if savepath in proc_files : return    # proc_files에는 처리된 파일명 입력 예정 -> 이미 처리된 파일인 경우 pass\n",
    "    proc_files[savepath] = True\n",
    "    print(\"analyze_html =\",url)\n",
    "    \n",
    "    # 링크 추출\n",
    "    html = open(savepath, \"r\", encoding='utf-8').read()\n",
    "    links = enum_links(html, url)\n",
    "    \n",
    "    for link_url in links :\n",
    "        # 링크가 루트경로(처음 시작 경로) 이외의 경로를 나타낸다면 무시하기\n",
    "        if link_url.find(root_url) != 0 :\n",
    "            if not re.search(r\".css$\", link_url) : continue\n",
    "        \n",
    "        # HTML이라면\n",
    "        if re.search(r\".(html|htm)$\", link_url) :\n",
    "            # 재귀적으로 html 파일 분석\n",
    "            analyze_html(link_url, root_url)\n",
    "            continue\n",
    "        # 기타 파일이라면\n",
    "        download_file(link_url)\n",
    "\n",
    "# url에 있는 모든 것 다운받기\n",
    "url = \"https://docs.python.org/3.10/library/\"\n",
    "analyze_html(url, url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\" :   # 일반 파이썬 파일인 경우 사용하는 코드\n",
    "#     # url에 있는 모든 것 다운받기\n",
    "#     url = \"https://docs.python.org/3.10/library/\"\n",
    "#     analyze_html(url, url)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ParseResult(scheme='https', netloc='example.com', path='/html/a.html', params='', query='', fragment='')\n",
      "example.com\n",
      "/html/a.html\n",
      "https://example.com/html\n"
     ]
    }
   ],
   "source": [
    "# 참고 ! netloc, path 등등 사용법\n",
    "url = \"https://example.com/html/a.html\"\n",
    "t = urlparse(url)\n",
    "print(t)        # ParseResult(scheme='https', netloc='example.com', path='/html/a.html', params='', query='', fragment='')\n",
    "print(t.netloc)     # example.com\n",
    "print(t.path)       # /html/a.html\n",
    "print(os.path.dirname(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### requests 패키지를 이용한 접근 (pip list를 통해 설치되어 있는지 확인)\n",
    "\n",
    "# 파이썬으로 로그인 해보기(는 실패,,, 보안이 약한 사이트 어디 없는쥐,,,,)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "\n",
    "# 아이디와 비밀번호 지정\n",
    "USER = \"wntpdls\"\n",
    "PASS = \"wntpdls\"\n",
    "\n",
    "# 세션 시작\n",
    "session = requests.session()\n",
    "# 로그인\n",
    "login_info = {\n",
    "    \"m_id\" : USER,\n",
    "    \"m_passwd\" : PASS\n",
    "}\n",
    "\n",
    "url_login = \"https://www.hanbit.co.kr/member/login.html\"\n",
    "res = session.post(url_login, data=login_info)\n",
    "res.raise_for_status()  # 오류 발생 시 예외 처리\n",
    "\n",
    "# 마이페이지 접근\n",
    "url_mypage = \"https://www.hanbit.co.kr/myhanbit/myhanbit.html\"\n",
    "res = session.get(url_mypage)\n",
    "res.raise_for_status()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requests 모듈의 메서드\n",
    "\n",
    "# get 요청\n",
    "r = requests.get(\"http://www.naver.com\")\n",
    "\n",
    "# post 요청\n",
    "formdata = {\"key1\":\"value1\",\"key2\":\"value2\"}\n",
    "r = requests.post(\"http://httpbin.org/\",data=formdata)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?????\n",
    "r = requests.put(\"https://webhacking.kr/put.txt\")      # bytes 데이터를 해당 url로 업로드\n",
    "print(r.text)\n",
    "r = requests.delete(\"http://httpbin.org/delete\")\n",
    "print(r.text)\n",
    "r = requests.head(\"https://jsonplaceholder.typicode.com/\")     # 헤더 정보만 받아옴(구글은 데이터 X)\n",
    "print(r.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tjoeun709\\AppData\\Local\\Temp\\ipykernel_2836\\1448183313.py:6: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(path)\n"
     ]
    }
   ],
   "source": [
    "# 테스트 창 열기\n",
    "from selenium import webdriver\n",
    "path = \"c:\\\\webdriver\\\\chromedriver.exe\"\n",
    "driver = webdriver.Chrome(path)\n",
    "\n",
    "# 한빛 로그인 페이지로 들어오기\n",
    "url_login = \"https://www.hanbit.co.kr/member/login.html\"\n",
    "driver.get(url_login)\n",
    "\n",
    "# 로그인 창에 ID와 비밀번호 입력\n",
    "USER = \"wntpdls\"\n",
    "PASS = \"wntpdls95\"\n",
    "\n",
    "m_id = driver.find_element_by_id('m_id')\n",
    "m_id.clear()\n",
    "m_id.send_keys(USER)\n",
    "m_passwd = driver.find_element_by_id('m_passwd')\n",
    "m_passwd.clear()\n",
    "m_passwd.send_keys(PASS)\n",
    "\n",
    "# 로그인 버튼 클릭\n",
    "driver.find_element_by_class_name('btn_login').click()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 마이페이지 접속\n",
    "driver.find_element_by_class_name('myhanbit').click()\n",
    "\n",
    "#url_mypage = \"https://www.hanbit.co.kr/myhanbit/myhanbit.html\"\n",
    "#driver.get(url_mypage)\n",
    "\n",
    "mileage = driver.find_element_by_css_selector('.mileage_section1 span')\n",
    "print('한빛 마일리지 :',mileage.text)\n",
    "\n",
    "coin = driver.find_element_by_css_selector('.mileage_section2 span')\n",
    "print('한빛 코인 :',coin.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 네이버 자동 로그인 해보기 - 보안 강해서 실패 ..\n",
    "\n",
    "driver.get('https://www.naver.com')\n",
    "driver.find_element_by_class_name('link_login').click()\n",
    "\n",
    "id = driver.find_element_by_id('id')\n",
    "id.clear()\n",
    "id.send_keys('sayin95')\n",
    "\n",
    "pw = driver.find_element_by_id('pw')\n",
    "pw.clear()\n",
    "pw.send_keys('wntpdls145')\n",
    "driver.find_element_by_class_name('btn_login').click()\n",
    "\n",
    "now = driver.current_url\n",
    "\n",
    "if 'nid.naver.com' in now :\n",
    "    while 1 :\n",
    "        pw = driver.find_element_by_id('pw')\n",
    "        pw.clear()\n",
    "        pw.send_keys('wntpdls145')\n",
    "\n",
    "        answer = input()\n",
    "        text = driver.find_element_by_id('captcha')\n",
    "        text.clear()\n",
    "        text.send_keys('answer')\n",
    "        \n",
    "        driver.find_element_by_class_name('btn_login').click()\n",
    "        time.sleep(5)\n",
    "        now = driver.current_url\n",
    "        if 'www.naver.com' in now : break\n",
    "\n",
    "\n",
    "driver.get(\"https://order.pay.naver.com/home\")\n",
    "points = driver.find_element_by_css_selector('.my_npoint strong')\n",
    "print('네이버페이 포인트 :', points.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SELinux : 리눅스의 보안 관련 기능 - 세션 정보를 유지시켜줌\n",
    "#             Enforcing : SELinux를 사용하면 SELinux 정책 규칙에 따라 액세스할 수 있습니다.\n",
    "#             Permissive : SELinux는 시행 모드에서 실행 중인 경우 거부되었을 작업만 기록합니다.\n",
    "#             Disabled : 로드된 SELinux 정책이 없습니다.  => 연습용으로 하둡을 가동할 때 꺼줬었음!\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
