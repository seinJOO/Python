{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7800a70",
   "metadata": {},
   "source": [
    "### 관련용어\n",
    "\n",
    "1. **크롤링(Crawling)**<br>\n",
    "\n",
    " 웹을 탐색하는 컴퓨터 프로그램(크롤러)을 이용하여 여러 인터넷 사이트의 웹페이지 자료를 수집하여 분류하는 과정\n",
    "\n",
    "\n",
    "2. **스크래핑(Scraping)**<br>\n",
    "\n",
    " 웹 사이트의 내용을 긁어다 원하는 형태로 가공하는 기술. 즉, 웹 사이트의 데이터를 수집하는 모든 작업을 의미\n",
    "\n",
    "\n",
    "3. **파싱(Parsing)**<br>\n",
    "\n",
    " 어떤 페이지(문서, html 등)에서 내가 원하는 데이터를 특정 패턴이나 순서로 추출하여 정보를 가공하는 것  \n",
    " 예) html소스를 문자열로 수집하여 html태그를 인식하도록 정보를 가공하여 html 단위별 분석이 가능하게 구성할 수 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c96bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'bytes'>\n",
      "<class 'str'>\n",
      "<class 'bs4.BeautifulSoup'>\n",
      "<h1 class=\"logo_default\">\n",
      "<a class=\"logo_naver\" data-clk=\"top.logo\" href=\"/\"><span class=\"blind\">네이버</span></a>\n",
      "</h1>\n",
      "--------------------------------------------------\n",
      "<a class=\"logo_naver\" data-clk=\"top.logo\" href=\"/\"><span class=\"blind\">네이버</span></a>\n",
      "--------------------------------------------------\n",
      "네이버\n",
      "--------------------------------------------------\n",
      "h2 태그 문자열들 출력 ======\n",
      "뉴스스탠드\n",
      "주제별 캐스트\n",
      "Sign in\n",
      "타임스퀘어\n"
     ]
    }
   ],
   "source": [
    "import urllib.request   # 웹에 통신 데이터를 요청하는 모듈\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "## 웹 상에 있는 html 소스 가져오기 (www.naver.com) = 스크래핑\n",
    "url = 'http://www.naver.com'\n",
    "resp = urllib.request.urlopen(url)\n",
    "#print(resp)      # <http.client.HTTPResponse at 0x1ae236082b0> => 데이터가 저장되어 있는 것을 알 수 있음\n",
    "\n",
    "data = resp.read()\n",
    "print(type(data))  # <class 'bytes'>\n",
    "#print(data)  # url로부터 가져온 html 소스를 byte 자료형으로 저장해옴\n",
    "\n",
    "\n",
    "## html 파싱\n",
    "html = data.decode('utf-8')     # byte -> 문서타입으로 변환\n",
    "print(type(html))  # str    - 문서타입으로 변환된 html소스가 출력됨\n",
    "soup = BeautifulSoup(html, 'html.parser')   # html 문서를 html 소스로 파싱\n",
    "print(type(soup))  # BeautifulSoup\n",
    "\n",
    "\n",
    "### 태그 내용 가져오기\n",
    "\n",
    "# 1. h1 태그 가져오기 (find() 메서드 사용 & 직접 참조)\n",
    "h1 = soup.find('h1')    # soup에 저장된 html 소스 중 가장 처음에 있는 h1태그에 대한 정보를 가져옴\n",
    "print(h1)               #<h1 class=\"logo_default\"><a class=\"logo_naver\" data-clk=\"top.logo\" href=\"/\"><span class=\"blind\">네이버</span></a></h1>\n",
    "print('-'*50)\n",
    "print(h1.a)             # <a class=\"logo_naver\" data-clk=\"top.logo\" href=\"/\"><span class=\"blind\">네이버</span></a>\n",
    "print('-'*50)\n",
    "print(h1.a.string)      # 네이버\n",
    "print('-'*50)\n",
    "\n",
    "# 2. find_all()\n",
    "h2s = soup.find_all('h2')   # :param name: A filter on tag name.\n",
    "type(h2s)        # bs4.element.ResultSet\n",
    "# print(h2s)       # type은 ResultSet이지만, list 형태로 데이터 처리됨\n",
    "\n",
    "print('h2 태그 문자열들 출력 ======')\n",
    "for h2 in h2s :\n",
    "    print(h2.string)        # h2태그 중 텍스트만 출력\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fba129",
   "metadata": {},
   "source": [
    "#### [문제] www.naver.com에서 a태그 정보를 수집하고 링크 문자열을 출력하는 코드를 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955cf418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "resp = urllib.request.urlopen('http://www.naver.com')\n",
    "data = resp.read()                          # byte타입으로 받음\n",
    "data = data.decode('utf-8')                 # 문서 타입으로 변환\n",
    "data = BeautifulSoup(data, 'html.parser')   # html소스 형태로 변환\n",
    "\n",
    "atags = data.find_all('a')\n",
    "print('='*20,'a태그 정보 모두 출력','='*20)\n",
    "for a in atags :\n",
    "    print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f4ace1b",
   "metadata": {},
   "source": [
    "#### [예제] 파일을 이용하여 html01.html을 읽어보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03536cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<h1> 시멘틱 태그 ?</h1>\n",
      " 주요 시멘틱 태그 \n",
      "body의 하위 태그들 모두 출력\n",
      "[<li> header : 문서의 머리말 (사이트 소개, 제목, 로그) </li>, <li> nav : 네비게이션 (메뉴) </li>, <li> section : 웹 문서를 장(chapter)으로 볼 때 절을 구분하는 태그 </li>, <li> aside : 문서의 보조 내용 (광고, 즐겨찾기, 링크) </li>, <li> footer : 문서의 꼬리말 (작성자, 저작권, 개인정보보호) </li>]\n",
      "liTags[0] : <li> header : 문서의 머리말 (사이트 소개, 제목, 로그) </li>\n",
      "liTags[1] : <li> nav : 네비게이션 (메뉴) </li>\n",
      "liTags[2] : <li> section : 웹 문서를 장(chapter)으로 볼 때 절을 구분하는 태그 </li>\n",
      "liTags[3] : <li> aside : 문서의 보조 내용 (광고, 즐겨찾기, 링크) </li>\n",
      "liTags[4] : <li> footer : 문서의 꼬리말 (작성자, 저작권, 개인정보보호) </li>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# html01 파일 읽기\n",
    "rfile = open('data/html01.html', encoding='utf-8')\n",
    "html01 = rfile.read()\n",
    "print(type(html01))    # <class 'str'> => open 메서드로 인해 읽으면서 바로 decoding 됨\n",
    "\n",
    "# html 파싱\n",
    "source = BeautifulSoup(html01,'html.parser')\n",
    "\n",
    "# h1 접근\n",
    "h1 = source.html.body.h1    # DOM 형식 (Document Object Mode - 계층적 구조로 접근)\n",
    "print(h1)\n",
    "h1.string\n",
    "\n",
    "# h2 접근\n",
    "h2 = source.find('h2')\n",
    "print(h2.string)\n",
    "\n",
    "# body 접근 - children\n",
    "body = source.html.body\n",
    "print('body의 하위 태그들 모두 출력')\n",
    "# for x in body.children : print(x)\n",
    "\n",
    "# li 태그들을 찾아서 출력하고, 내용을 출력\n",
    "lis = source.find_all('li')\n",
    "print(lis)\n",
    "idx = 0\n",
    "for x in lis :\n",
    "    print(f'liTags[{idx}] : {x}')\n",
    "    idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36a9eb79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "links size = 5\n",
      "<a href=\"www.naver.com\">네이버</a>\n",
      "www.naver.com\n",
      "예외 발생 : 'target'\n",
      "<a href=\"'http://www.naver.com\">네이버</a>\n",
      "'http://www.naver.com\n",
      "예외 발생 : 'target'\n",
      "<a href=\"http://www.naver.com\" target=\"_blank\">네이버 새창으로</a>\n",
      "http://www.naver.com\n",
      "_blank\n",
      "<a href=\"'www.daum.net\">다음</a>\n",
      "'www.daum.net\n",
      "예외 발생 : 'target'\n",
      "<a href=\"http://www.daum.net\">다음</a>\n",
      "http://www.daum.net\n",
      "예외 발생 : 'target'\n",
      "패턴 객체를 생성하여 속성 찾기\n",
      "[<a href=\"'http://www.naver.com\">네이버</a>, <a href=\"http://www.naver.com\" target=\"_blank\">네이버 새창으로</a>, <a href=\"http://www.daum.net\">다음</a>]\n",
      "[<a href=\"http://www.naver.com\" target=\"_blank\">네이버 새창으로</a>]\n"
     ]
    }
   ],
   "source": [
    "### 태그 속성 찾기\n",
    "# 파일 읽기\n",
    "rfile2 = open('data/html02.html','r',encoding='utf8')\n",
    "src2 = rfile2.read()\n",
    "\n",
    "# html 파싱\n",
    "html02 = BeautifulSoup(src2, 'html.parser')\n",
    "\n",
    "# a tag 찾기\n",
    "links = html02.find_all('a')    # a태그 모두 찾기\n",
    "print('links size =',len(links))\n",
    "\n",
    "# a tag에서 속성 찾기\n",
    "for link in links :\n",
    "    try :\n",
    "        print(link)                     # <a href=\"www.naver.com\">네이버</a>\n",
    "        print(link.attrs['href'])       # www.naver.com >> DOM 구조로 접근 (a.href - a태그의 href속성)\n",
    "        print(link.attrs['target'])     # _blank >>> DOM 구조로 접근 (a.target - a태그의 target속성)\n",
    "    except Exception as e :\n",
    "        print('예외 발생 :',e)            # 예외 발생 : 'target' >> target 속성이 없을 때 이렇게 처리됨\n",
    "        \n",
    "        \n",
    "### 정규표현식으로 속성 찾기\n",
    "\n",
    "import re\n",
    "print('패턴 객체를 생성하여 속성 찾기')\n",
    "patt = re.compile('http://')            # patt 객체 생성\n",
    "links = html02.find_all(href=patt)      # 패턴 찾기\n",
    "print(links)    # 'http://'가 포함된 href속성을 가진 a태그만 출력    \n",
    " \n",
    "links = html02.find_all(target='_blank')    # 패턴 찾기\n",
    "print(links)    # target속성의 값이 '_blank'인 a태그만 출력\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431fd7e",
   "metadata": {},
   "source": [
    "#### [문제] www.naver.com에서 a태그 정보를 수집하고 링크 문자열을 출력하는 코드를 작성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e73eaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "resp = urllib.request.urlopen('http://www.naver.com')\n",
    "data = resp.read()                          # byte타입으로 받음\n",
    "data = data.decode('utf-8')                 # 문서 타입으로 변환\n",
    "data = BeautifulSoup(data, 'html.parser')   # html소스 형태로 변환\n",
    "\n",
    "atags = data.find_all('a')\n",
    "print('='*20,'a태그의 href 정보 모두 출력','='*20)\n",
    "idx = 0\n",
    "for a in atags :\n",
    "    try :\n",
    "        print(idx,'번째',end=', ')\n",
    "        print(a.string, end=', ')\n",
    "        print(a.attrs['href'])\n",
    "        idx += 1\n",
    "    except Exception as e :\n",
    "        print('예외 발생 :',e)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30cb2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# html03.html에서 선택자를 이용한 정보 수집\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 로컬 파일 읽기\n",
    "rfile3 = open('data/html03.html','r',encoding='utf-8')\n",
    "src3 = rfile3.read()\n",
    "\n",
    "# html 파싱\n",
    "html3 = BeautifulSoup(src3, 'html.parser')\n",
    "\n",
    "# 선택자를 이용한 내용 가져오기\n",
    "\n",
    "# 1. id = 'tab' 선택자 정보 가져오기\n",
    "print('>>> table 선택자 <<<')\n",
    "\n",
    "table = html3.select_one('#tab')      # table에 있는 id='tab'으로 접근\n",
    "                # .select(selector) : 모두 가져옴 / .select_one(selector) : 가장 처음에 있는 하나만 가져옴 / selector : css의 selector\n",
    "                \n",
    "#print(table)    # table 태그 전체 출력\n",
    "\n",
    "# 2. id 선택자와 계층\n",
    "print('>>> 선택자 & 계층 <<<')\n",
    "ths = html3.select('#tab > tr > th')\n",
    "print(th)       # list 형태로 반환\n",
    "\n",
    "# 3. class 선택자\n",
    "print('>>> class 선택자 <<<')\n",
    "trs = html3.select('#tab > .odd')\n",
    "print(trs)\n",
    "print('>>> 속성값을 이용한 선택 <<<')\n",
    "trs2 = html3.select(\"tr[class='odd']\")\n",
    "print(trs2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "42ccbed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 201601 \n",
      " 홍길동 \n",
      " 체육학과 \n",
      " hong@naver.com \n",
      " 201602 \n",
      " 이순신 \n",
      " 해양학과 \n",
      " lee@naver.com \n",
      " 201603 \n",
      " 강감찬 \n",
      " 정치외교 \n",
      " kang@naver.com \n",
      " 201604 \n",
      " 유관순\n",
      " 유아교육 \n",
      " you@naver.com \n"
     ]
    }
   ],
   "source": [
    "# [예제] tr > td에 있는 문자열을 출력하는 코드\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 로컬 파일 읽기\n",
    "rfile3 = open('data/html03.html','r',encoding='utf-8')\n",
    "src3 = rfile3.read()\n",
    "\n",
    "# html 파싱\n",
    "html3 = BeautifulSoup(src3, 'html.parser')\n",
    "strs = html3.select('tr > td')\n",
    "\n",
    "# 문자열 추출하여 출력\n",
    "for x in strs :\n",
    "    print(x.string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff0d02de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [실습] : www.naver.com, www.daum.net, www.yahoo.com에서 li태그 목록 정보를 불러와 출력\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'http://www.naver.com'\n",
    "# url = 'https://news.daum.net/culture#1'\n",
    "# url = 'http://www.yahoo.com'\n",
    "\n",
    "resp = urllib.request.urlopen(url)\n",
    "data = resp.read()\n",
    "data = data.decode('utf-8')\n",
    "data = BeautifulSoup(data,'html.parser')\n",
    "lis = data.find_all('li')\n",
    "for x in lis :\n",
    "    print(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b893d652",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "제목 : 박정희 시대의 '새마을'…마을 단위 감시체제를 만들다\n",
      "내용 : 새마을이라는 말을 들으면 새마을운동을 떠올리기 쉽다. 박정희 정부가 1970년대 시동을 건 새마을운동은 농촌 근대화 운동으로 널 …\n",
      "\n",
      "제목 : 만두 먹으며 꺼낸 추억 ‘아무 걱정 없이, 오늘도 만두’ [서평]\n",
      "내용 : (서울=뉴스1) 최수아 디자이너 = 만두를 먹는 행위는 작가에게 ‘위로’가 되기도 하고 소망을 비는 ‘제사’이기도 하다. 작가에 …\n",
      "\n",
      "제목 : 우연한 만남과 헤어짐, 24년뒤 다시 그리움 ‘그해, 선셋 비치에서’[서평]\n",
      "내용 : (서울=뉴스1) 이지원 디자이너 = 우리는 종종 어떻게 하면 우리 주변의 행복한 커플처럼 서로를 사랑하는 데 노력이 필요한 것  …\n",
      "\n",
      "제목 : [신간] 오래되고 멋진 클래식 레코드\n",
      "내용 : 블루칼라 보수주의·롤랑의 노래·오늘도 애쓰셨습니다 (서울=연합뉴스) 김계연 기자 = ▲ 오래되고 멋진 클래식 레코드 = 무라카미 …\n",
      "\n",
      "제목 : No人·어르신 아닌 '선배시민'으로서의 자아 찾기\n",
      "내용 : 한국 노인은 벼랑 끝에 서 있다. 노인 상대 빈곤율은 43.4%(2018년 기준)에 달하고, 늙어서도 일해야 먹고살 수 있기 때 …\n",
      "\n",
      "제목 : [신간] 미중 디지털 패권경쟁\n",
      "내용 : 두려움 없이, 당신 자신이 되세요·사소한 기쁨 우리 모두의 마음속에는 길을 잃고 헤매는 이가 있다 (서울=연합뉴스) 임형두 기자 …\n",
      "\n",
      "제목 : 플라톤부터 윤동주까지…'철학자의 사랑법'\n",
      "내용 : [서울=뉴시스]신재우 기자 = \"인간이 인간을 사랑하는 것, 이것은 어쩌면 우리에게 가장 어려운 일입니다.\" 철학자의 눈으로 바 …\n",
      "\n",
      "제목 : 악보에 새긴 시대의 격랑…신간 '역사를 만든 음악가들'\n",
      "내용 : (서울=연합뉴스) 김계연 기자 = 프랑스혁명이 일어났을 때 베토벤은 독일 본에서 문학을 공부하며 음악을 만들고 있었다. 혁명의  …\n",
      "\n",
      "제목 : Meeting (회의)\n",
      "내용 : [영어로 즐기는 만화, JACK OF ALL TRADES-1454] \n",
      "\n",
      "제목 : [책꽂이] “쾌락 과잉 시대에 적당한 고통은 행복의 동반자”\n",
      "내용 : [서울경제] 19살 케빈은 학교를 가려 하지 않았고 가정 내 규칙도 지키지 않았다. 부모는 아들에게 정신적 고통이나 스트레스를  …\n",
      "\n",
      "제목 : 문 대통령 연설문집 베스트셀러 5위 돌풍… 조국은 2주연속 1위\n",
      "내용 : [서울경제] 조국 전 법무부 장관의 신간 ‘가불 선진국’(假拂 先進國)이 2주 연속 베스트셀러 종합 1위를 기록했다. 문재인 대 …\n",
      "\n",
      "제목 : 2022년 책읽는청주 3권 선정…'아픔이 길이 되려면' 등\n",
      "내용 : 기사내용 요약 청소년 '불편한 편의점' 아동 '강남 사장님' [청주=뉴시스] 임선우 기자 = 충북 청주시는 '2022년 책 읽는 …\n",
      "\n",
      "제목 : 부커상 후보 정보라 '저주토끼', 5년 만에 재출간\n",
      "내용 : [서울=뉴시스]신재우 기자 = “용서할 수 없는 것들은, 용서하지 말자.” 2022 부커상 1차 후보 지명과 함께 정보라의 소설 …\n",
      "\n",
      "제목 : OTT 인기, 서점가 강타… 애플TV+ '파친코' 원작소설, 판매량 급증\n",
      "내용 : 애플TV+의 오리지널 시리즈 '파친코'의 인기에 원작 소설인 '파친코'의 판매량 역시 크게 증가하고 있는 것으로 나타났다. 알라 …\n",
      "\n",
      "제목 : 그때는 틀리고 지금은 맞다… 워런 버핏이 돈 버는 진짜 이유 [읽어본다, 과학]\n",
      "내용 : 편집자주 어렵고 낯선 과학책을 수다 떨 듯 쉽고 재미있게 풀어냅니다. ‘읽어본다, SF’를 썼던 지식큐레이터(YG와 JYP의 책 …\n",
      "\n",
      "제목 : 중독에서 벗어나고 싶다면… \"고통을 직면하라\"\n",
      "내용 : 술, 게임, 담배, 도박, 사회관계망서비스(SNS)… 모두 신경전달물질인 도파민을 일으키는 대상이다. 도파민은 신경과학에서 중독 …\n",
      "\n",
      "제목 : 2022년 우수 납본 출판사에 ‘위즈덤하우스·북이십일’\n",
      "내용 : [이데일리 김미경 기자] 국립중앙도서관은 올해 우수 납본 출판사로 ㈜위즈덤하우스(일반도서 부문)와 ㈜북이십일(온라인자료 부문)을 …\n",
      "\n",
      "제목 : [책 한 모금] 프랑스 유럽 정치학자들이 들려주는 ‘만화로 보는 좌파의 역사’\n",
      "내용 : 그 자체로 책 전체 내용을 함축하는 문장이 있는가 하면, 단숨에 독자의 마음에 가닿아 책과의 접점을 만드는 문장이 있습니다. 책 …\n",
      "\n",
      "제목 : 보수주의는 어떻게 노동계급의 환심을 샀나\n",
      "내용 : 2020년 흑인 남성 조지 플로이드 사망에 항의하는 인종 차별 반대 시위를 계기로 미국 곳곳에서 인종 차별적 인물의 동상이 철거 …\n",
      "\n",
      "제목 : 日 반발에도 흥행질주 '파친코', 원작소설 불티...판매량 15배↑\n",
      "내용 : 애플TV+ 웹드라마 '파친코'가 연일 화제가 되면서 원작 소설의 인기도 크게 늘어나고 있다. 31일 인터넷 서점 '알라딘'에 따 …\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [실습 2] 뉴스 주요 기사 내용 출력\n",
    "\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "url = 'https://news.naver.com/main/list.naver?mode=LS2D&mid=shm&sid1=103&sid2=243'\n",
    "\n",
    "resp = urllib.request.urlopen(url)\n",
    "data = resp.read()\n",
    "data = data.decode('euc-kr')\n",
    "data = BeautifulSoup(data,'html.parser')\n",
    "#print(data)\n",
    "data = data.select('.list_body li')\n",
    "\n",
    "for x in data :\n",
    "    title = x.select_one('img')\n",
    "    print('제목 :',title.attrs['alt'])\n",
    "    text = x.select_one('dd > .lede')\n",
    "    print('내용 :',text.string)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
