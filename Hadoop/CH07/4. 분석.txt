pdf 비밀번호 : techbig1

기술 분석: 분석 초기 데이터의 특징을 파악하기 위해 선택, 집계, 요약 등 양적 기술 분석을 수행
탐색 분석: 업무 도메인 지식을 기반으로 대규모 데이터셋의 상관관계나 연관성을 파악
추론 분석: 전통적인 통계분석 기법으로 문제에 대한 가설을 세우고 샘플링을 통해 가설을 검증
인과 분석: 문제 해결을 위한 원인과 결과 변수를 도출하고 변수의 영향도를 분석
예측 분석: 대규모 과거 데이터를 학습해 예측 모형을 만들고, 최근의 데이터로 미래를 예측

스마트카 빅데이터 분석에 SNS, 포털, 날씨, 뉴스, 위치 정보 등이 결합된다면 스마트카 사용자들이
SNS에 남긴 라이프로그와 운행 패턴과의 연관성을 분석해 매우 정밀한 고객 세그먼테이션 및 프로
파일링으로 차별화된 타겟 마케팅이 가능

<< 임팔라 >> - 실시간 빅데이터 분석 질의 가능
- 하이브 대비 빠른 응답 속도를 보장 but 많은 리소스 사용, 특히 높은 메모리 사용률(중간 데이터셋을 모두 메모리에 올려놓고 작업)
    -> 대규모 배치 작업에서는 안정적인 하이브, 업무시간 중의 애드혹 탐색적 분석에서는 임팔라
- 하이브 쿼리를 임팔라 쿼리로 바꾸고, 스마트카 데이터셋을 실시간 탐색    
    ✔ 임팔라 - 스마트카 상태정보 On-Line 조회
    ✔ 휴 스마트카 - 운전자 운행정보 On-Line 조회

<< 제플린 >> - Notebook
- 대용량 데이터셋을 빠르게 파악하고 이해하기 위한 분석 및 시각화 툴 (R, ...)
- 분산 병렬처리를 안정적으로 할 수 있도록 다양한 인터프리터를 제공 (스파크 기반)
- 5개의 마트 데이터를 대상으로 제플린에서 스파크-SQL을 이용해 다양한 애드혹 분석을 수행
    Notebook : ✔ 상습 과속 격을 Spark-SQL 분석 √ 분석 결과를 제플린 차트로 출력
    분류 라이브러리 : √ 머신러닝 감독학습 √ 랜덤 포레스트 알고리즘 활용 ✔ 스마트카 상태정보 예측 모델
    군집 라이브러리 : √ 머신러닝 비감독학습 ✔ Canopy / K-Means 알고리즘 활용 √ 스마트카 고객 성향 분석

<< 머하웃 >>
- 머신러닝 기법을 이용해 데이터 마이닝을 수행하는 툴
- 주요 라이브러리 : 추천, 분류, 군집
- 추천 라이브러리를 활용해 “차량용품 구매 이력 데이터를 분석하고 스마트카 운전자 가운데 유사 그룹 간의 구매 선호도에 따라 차량용품을 추천하는 작업
    ✔ 스마트카 사용자 기반 협업 필터링
    ✔ 사용자 간 유사성으로 상품을 추천

<< 스쿱 >> - 단순한 import, export 작업 수행
- 특별한 전처리 없이 곧바로 HDFS에 적재하거나, 반대로 HDFS에 저장된 데이터를 외부 RDBMS로 제공
-  하이브, 임팔라, 제플린, 머하웃 등에서 분석된 결과를 외부 RDBMS 시스템에 편리하게 제공하기 위한 도구로 활용

# 현재의 스마트카 데이터셋
    주제 영역 1: 스마트카 상태 모니터링 정보
    주제 영역 2: 스마트카 운전자 운행기록 정보
    주제 영역 3: 이상 운전 패턴 스마트카 정보
    주제 영역 4: 긴급 점검이 필요한 스마트카 정보
    주제 영역 5: 운전자의 차량용품 구매 이력 정보
    주제 영역 +: 스마트카 고급 분석 결과

서비스 추가 (17p~)
임팔라 설치 - 스쿱 설치 - 제플린 설치 - 머하웃 설치

<< 임팔라를 이용한 운행 지역 분석 >>
1. 이상 운전패턴 스마트카 정보 조회
select * from Managed_SmartCar_Symptom_Info where biz_date = '20220429'

2. 긴급 점검이 필요한 스마트카 정보 조회
select * from managed_smartcar_emergency_check_info where biz_date = '20220429'

3. 스마트카 차량용품 구매 이력 정보 조회
select * from managed_smartcar_item_buylist_info where biz_month = '202204'

4. 지역별로 160km/h 이상의 평균 속도로 과속하는 상습차량 조회
    select
        T2.area_number, T2.car_number, T2.speed_avg
     from (
            select 
                    T1.area_number, T1.car_number, T1.speed_avg,
                    rank() over(partition by T1.area_number order by T1.speed_avg desc) as ranking
            from    (
                    select area_number, car_number, avg(cast(speed as int)) as speed_avg
                    from managed_smartcar_drive_info
                    group by area_number, car_number
                    ) T1
        )T2
     WHERE ranking = 1;

<< 제플린을 이용한 운행 지역 분석 >>
1. 제플린 구동 후 server02.hadoop.com:8081에 접속
   $ zeppelin-daemon.sh start

2. 제플린 NoteBook인 SmartCar-Project 생성 (기본 인터프리터 : 스파크)

3. 명령어를 실행해 분석할 스마트카 운전자의 운행데이터 확인 (head명령어로 10줄만)
    %sh     >> 노트북에서 셸 명령이 가능하도록 바인딩하는 명령어
    hdfs dfs -cat /user/hive/warehouse/managed_smartcar_drive_info/biz_date=20220429/* | head

4. 스칼라 코드 작성 (HDFS에서 데이터를 로드 및 로드한 데이터셋을 스파크에서 활용할 수 있는 데이터 구조로 만들기)
   여기서 클래스는 DTO/VO와 같은 기능을 함 / CSV형태의 파일이기 때문에 ',' 기준으로 split
    val url="hdfs://server01.hadoop.com:8020"
     val dPath="/user/hive/warehouse/managed_smartcar_drive_info/biz_date=20220429/*"
     val driveData=sc.textFile(url + dPath)
     case class DriveInfo(car_num: String, sex: String, age: String, marriage: String, region: String, job: String,           
                        car_capacity: String, car_year: String, car_model: String, speed_pedal: String, break_pedal: String, steer_angle: String, 
                        direct_light: String, speed: String, area_num: String, date: String)

     val drive = driveData.map(sd=>sd.split(",")).map(
                    sd=>DriveInfo(sd(0).toString, sd(1).toString, sd(2).toString, sd(3).toString,
                                sd(4).toString, sd(5).toString, sd(6).toString, sd(7).toString,
                                sd(8).toString, sd(9).toString, sd(10).toString,sd(11).toString,
                                sd(12).toString,sd(13).toString,sd(14).toString,sd(15).toString
            )
     )
     drive.toDF().registerTempTable("DriveInfo")

5. 스파크-SQL 실행
    %spark.sql
     select T1.area_num, T1.avg_speed
     from (select area_num, avg(speed) as avg_speed from DriveInfo group by area_num) T1
     order by T1.avg_speed desc

6. 위 쿼리 + 평균속도(AvgSpeed 동적변수 정의) 조건 추가 => 쿼리창에 직접 변수값 빠르게 입력 가능
    %spark.sql
     select T1.area_num, T1.avg_speed
     from (select area_num, avg(speed) as avg_speed from DriveInfo group by area_num having avg_speed >= ${avg_speed=60} ) T1
     order by T1.avg_speed desc




%spark
import org.apache.spark.ml.feature.StringIndexer
val dsSmartCar_1 = new StringIndexer().setInputCol("car_model").setOutputCol("car_model_n").fit(dsSmartCar).transform(dsSmartCar)
val dsSmartCar_2 = new StringIndexer().setInputCol("engine").setOutputCol("engine_n").fit(dsSmartCar_1).transform(dsSmartCar_1)
val dsSmartCar_3 = new StringIndexer().setInputCol("break").setOutputCol("break_n").fit(dsSmartCar_2).transform(dsSmartCar_2)
val dsSmartCar_4 = new StringIndexer().setInputCol("status").setOutputCol("status_n").fit(dsSmartCar_3).transform(dsSmartCar_3)
val dsSmartCar_5 = dsSmartCar_4.drop("car_model").drop("engine").drop("break").drop("status")
dsSmartCar_5.show()