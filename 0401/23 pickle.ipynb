{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [실습 3] 크롤링 데이터를 파일로 만들기 (open()을 활용하여 기존에 배운대로)\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = 'https://news.daum.net/'\n",
    "data = urllib.request.urlopen(url)              # url을 파일로 오픈\n",
    "data = data.read()                              # 바이트 형태로 파일 읽기\n",
    "source = data.decode('utf-8')                   # 디코딩\n",
    "htmls = BeautifulSoup(source, 'html.parser')    # html로 파싱\n",
    "\n",
    "hdls = htmls.select('.tit_g > a')   # 리스트 형태로 저장됨\n",
    "print(type(hdls))   # <class 'bs4.element.ResultSet'>\n",
    "\n",
    "crawling_data = []\n",
    "\n",
    "i = 1\n",
    "for x in hdls :\n",
    "    if x.string :\n",
    "        # print(f'{i}번째 헤드라인 :',x.string.strip())\n",
    "        crawling_data.append(x.string.strip())\n",
    "        i += 1\n",
    "print(crawling_data)\n",
    "\n",
    "# text save : object -> file(string) -> load(string)\n",
    "\n",
    "sfile = open('data/crawling_data.txt','w',encoding='utf-8')\n",
    "for x in crawling_data :\n",
    "    sfile.write(x+'\\n')\n",
    "sfile.close()\n",
    "print('\\n저장한 크롤링 데이터 불러오기================\\n')\n",
    "rfile = open('data/crawling_data.txt',encoding='utf-8')\n",
    "print(rfile.read())\n",
    "rfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pickle    (내장모듈)\n",
    "# 1. import pickle을 통해서 모듈을 임포트\n",
    "# 2. pickle 모듈을 이용하면 원하는 데이터를 자료형의 변경 없이 파일에 그대로 저장하고, 그대로 로드하여 사용 가능\n",
    "#       -> 딕셔너리 자료형의 경우 key, value값을 unpacking하여 저장해야하는데, 피클을 사용하면 필요없음!\n",
    "# 3. pickle로 데이터를 저장하거나 불러올 때에는 파일을 바이트 형식으로 읽거나 써야한다\n",
    "#    wb모드(write bytes mode)로 데이터를 입력하는 경우, 확장자는 .bin을 사용하는 게 좋음\n",
    "# 4. 모든 파이썬 데이터 객체를 저장하고 읽을 수 있음\n",
    "\n",
    "\n",
    "## 저장(save)\n",
    "# pickle.dump(data, file) -> data 저장한 소스 데이터 버퍼, file은 저장할 위치(파일)\n",
    "\n",
    "# (형식 예시) \n",
    "# import pickle\n",
    "# list = [1,2,3,4,5]\n",
    "# with open('list.bin','wb') as f:          # with로 open할 파일 정의, f로 명명하여 작업 진행\n",
    "#       pickle.dump(list, f)                # 데이터 입력작업(dump) 후 자동으로 close\n",
    "                                            # => open(), read(), close()를 pickle로 한 번에 작업 가능함 !!!\n",
    "# (기존 방법으로 하자면)\n",
    "# file = open('list.bin','wb')\n",
    "# list = [1,2,3,4,5]\n",
    "# pickle.dump(list, file)\n",
    "# file.close()\n",
    "\n",
    "\n",
    "## 로드(load)\n",
    "\n",
    "# (형식 예시)\n",
    "# import pickle\n",
    "# var = pickle.load(file)\n",
    "# with open('list.bin','rb') as f :\n",
    "#       var = pickle.load(f)        # 한 줄씩 데이터를 읽어와서 var에 저장함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [예제] 피클을 이용한 데이터 저장\n",
    "import pickle\n",
    "lst = [1,2,3,4,5]\n",
    "with open('data/lst.bin','wb') as f :\n",
    "    pickle.dump(lst, f)             # 바이너리로 데이터 입력 및 저장 후 close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5]\n"
     ]
    }
   ],
   "source": [
    "# [예제] 피클을 이용한 데이터 로드\n",
    "import pickle\n",
    "with open('data/lst.bin','rb') as f :   # 바이너리값으로 데이터 읽기\n",
    "    data = pickle.load(f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pickle로 save 완료 !\n",
      "['\"尹 정부 \\'중소기업 성장사다리\\' 복원, 지원 일색 정책으론 안돼\"', \"'잭팟' 터뜨린 두나무 공동 창업자, 일선에서 물러나는 이유는\", '쿠팡이츠 배달노동자들 \"사고 나도 산재보험 미적용 대다수\"', \"도심 속 친환경 공원 품은 '힐스테이트 창원 더퍼스트' 분양 예정\", '\"남들은 다 올리는데\"..오리온, 9년째 가격 동결 비결은?', \"과거엔 美 금리 올리면 달러 떨어졌는데..'이번엔 다르다' 전망\", \"정부, '국산 1호' 백신 하반기 사용 가능하도록 적극 지원한다\", \"3월 수출·수입 '역대 최고'..유가 급등 무역수지는 다시 적자(종합2보)\", '박두선 대우조선해양 대표이사, 조선업계 CEO 포럼 불참', \"'오픈런' 없어진다?..보복소비 사라질까, 백화점 업계 긴장\", '\"해외여행 해볼까\"..여행 예약 늘고, 면세품 구매도 증가', '핌코-아문디 \"미국 2년 안에 침체 빠질 가능성 낮다\"', '한국조선해양플랜트협회 \"인력확보 위한 민·관 협력 플랫폼 필요\"', '[장중시황] 코스피, 2740선 하락세..외인·기관 동반 매도', '\"올 것이 왔다\"..포스코 출근 재개에 재계 \\'술렁\\'', '김기문 중기중앙회장 \"尹 정부 최우선 과제는 양극화 해결\"', '물가 부담에 통신비 낮춘다..\"알뜰폰 요금 추가 인하\"', '3년 새 당기순익 4배·영업익3배..DB하이텍의 역습', \"36조원 스트리밍 시장 향하는 中 '규제 칼날'\", \"'26분 컷' 속도 무엇?..올리브영 강남 물류센터 가보니\", \"'깜짝 대박' 가상자산거래소..이제는 기능·책임 강화를\", '韓 퇴직연금 수익률 2%대 선진국의 3분의 1 수준인 이유', \"SK 손잡고 도시바메모리 인수한 베인, 이번엔 도시바 '눈독'\", '지역 간 격차, 지역 회복력이 우선돼야', '美하루 100만 배럴 비축유 방출..사상 최대']\n"
     ]
    }
   ],
   "source": [
    "# [실습 3] 크롤링 데이터를 파일로 만들기 (피클 활용하기)\n",
    "import urllib\n",
    "from bs4 import BeautifulSoup\n",
    "import pickle\n",
    "\n",
    "url = 'https://news.daum.net/economic#1'\n",
    "data = urllib.request.urlopen(url)              # url을 파일로 오픈\n",
    "data = data.read()                              # 바이트 형태로 파일 읽기\n",
    "source = data.decode('utf-8')                   # 디코딩\n",
    "htmls = BeautifulSoup(source, 'html.parser')    # html로 파싱\n",
    "hdls = htmls.select('.tit_g > a')               # 리스트 형태로 저장됨\n",
    "crawling_data = []\n",
    "\n",
    "i = 1\n",
    "for x in hdls :\n",
    "    if x.string :\n",
    "        crawling_data.append(x.string.strip())\n",
    "        i += 1\n",
    "\n",
    "# text save : object -> file(string) -> load(string)\n",
    "\n",
    "with open('data/data.pickle1','wb') as file :      # 이진바이너리로 저장되기 때문에 확장명 아무거나 사용가능\n",
    "    pickle.dump(crawling_data, file)\n",
    "    print('pickle로 save 완료 !')\n",
    "\n",
    "with open('data/data.pickle1','rb') as file :\n",
    "    crawling_data = pickle.load(file)\n",
    "print(crawling_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " >> 단어 개수 카운트 <<\n",
      "{'정경심': 4, '뇌종양': 2, '뇌경색': 1, '진단영장': 1, '청구': 2, '변수': 1, '되나': 1, '최근': 1, '뇌종양뇌경색': 2, '진단': 2, '새': 1, '변수로': 1, '지옥의': 2, '고통도': 2, '짧다': 2, '정경심은': 2, '왜': 2, '박노해': 2, '시로': 2, '조국': 10, '받은': 1, '정경심검찰': 1, '불구속': 1, '기소': 1, '측': 1, '백지': 1, '공소장으론': 1, '재판': 2, '못': 2, '한다': 1, '전': 3, '장관': 3, '서울대': 4, '복직동문': 1, '온라인': 2, '커뮤니티': 2, '시끌': 1, '사퇴': 2, '하루': 1, '만에': 2, '복직학내': 1, '논란': 1, '대한민국': 1, '뒤집어': 1, '놓고서울대': 1, '복직': 3, '한국당': 1, '소식에서울대': 1, '찬반': 1, '투표': 1, '오늘': 1, '법학전문대학원': 1, '교수직': 1, '지하철': 4, '노사': 2, '막판': 3, '줄다리기세': 1, '가지': 1, '핵심': 1, '쟁점은': 1, '호선': 1, '협상결렬': 1, '시': 1, '일부터': 1, '총': 1, '일': 1, '서울': 2, '출근길': 1, '대란': 1, '현실화되나노사': 1, '내일': 1, '파업하나': 1, '마지막': 1, '협상': 1, '돌입': 1, '서울지하철': 1, '파업': 1, '밤샘': 1, '협상교통혼란': 1, '예고': 1, '일본': 2, '안': 1, '가고': 1, '홍콩': 1, '가고여행업계': 1, '가을': 1, '성수기도': 1, '시름': 1, '유니클로': 1, '야나이': 1, '회장': 1, '이대로': 1, '가면': 1, '일본은': 1, '망할': 1, '것': 1, '일본산': 1, '맥주': 1, '외면옛': 1, '명성': 1, '회복': 1, '불능': 1, '서경덕': 1, '교수가': 1, '말하는': 1, '역사': 1, '왜곡': 1, '바로잡기': 1, '재팬': 1, '일유니클로': 1, '년': 1, '첫': 1, '매출': 1, '억울한': 1, '옥살이': 1, '또년': 1, '복역했는데': 1, '이춘재': 1, '짓': 1, '화성': 2, '사건': 2, '이춘재가': 1, '자백한': 1, '여죄': 2, '건은': 1, '범죄의': 1, '재구성화성사건에': 1, '더하니': 1, '냉각기': 1, '박준영': 1, '변호사': 1, '화성살인': 1, '차': 1, '정보공개': 2, '요청': 1, '차사건': 1, '재심': 1, '변호인': 1, '수사기록': 1, '시진핑': 1, '해양경제': 1, '발전': 1, '성과는': 1, '전세계와': 1, '나눠야': 1, '日환경상': 1, '태풍': 1, '유실': 1, '방사성': 1, '폐기물': 2, '환경에': 1, '영향': 1, '없다': 1, '檢': 1, '다음': 1, '칼': 1, '끝은': 1, '한국당소환': 1, '않고': 1, '넘길까': 1, '후임자': 1, '찾기': 1, '착수한': 1, '靑중폭개각靑개편': 1, '가능성도': 1, '솔솔': 1, '석유공사': 1, '모르쇠에': 1, '수백억': 1, '날려국감서': 1, '울분': 1, '표한': 1, '중기': 1, '대표': 1, '멧돼지': 1, '소탕': 1, '군사작전더': 1, '빨리': 1, '확산될': 1, '수도': 1, '文의장': 1, '세르비아': 1, '대통령에게': 1, '한반도': 1, '프로세스': 1, '지지': 1, '당부': 1, '쇼핑': 1, '거래액': 1, '조엄지족': 1, '잡기': 1, '나선': 1, '유통가': 1, '방사능': 1, '태평양': 1, '흘러갔을지': 1, '모르는데': 1, '日보도규제': 1, '하는': 1, '애완견과': 1, '함께': 1, '택시': 1, '타려다': 1, '강제하차': 1, '봉변기사': 1, '벌금형': 1, '선택적': 2, '정의': 1, '수사': 1, '윤석열의': 1, '검찰': 1, '개혁될까': 1, '없이': 1, '진행된': 1, '법무부': 2, '국감': 1, '종료김': 1, '빠진': 1, '여야': 1, '공방': 1, '사회': 1, '이모저모': 1, '대통령': 1, '부인': 1, '김정숙': 2, '여사': 1, '카탈루냐': 1, '분리독립': 1, '추진': 1, '전국은': 1, '지금': 1, '설리': 3, '사망에도': 1, '악플은': 1, '멈추지': 1, '않는다': 1, '[단독]청소년센터서': 1, '정신과': 1, '약물': 1, '강제복용': 1, '가장': 2, '폭력적인': 1, '곳에서': 1, '전투적으로': 1, '싸웠던': 1, '[기자메모]': 1, '심경': 1, '전했나': 1, '재산': 1, '빼돌려': 1, '호화생활': 1, '악의적': 1, '고액상습': 1, '체납자': 1, '집중': 1, '타깃': 1, '[국세청': 1, '체납': 1, '세금과의': 1, '전쟁]': 1, '조카와': 1, '놀아주던': 1, '흑인여성': 1, '사살한': 1, '美백인경찰관': 1, '살인혐의': 1, '기소종합': 1, '버스': 1, '앞좌석': 1, '승객': 1, '머리에': 1, '체액뿌린': 1, '남성': 1, '항소심': 1, '무죄': 1, '반전': 1, '이철희': 2, '검사': 1, '블랙리스트': 1, '작성': 1, '주장대검은': 1, '반박종합': 1, '코스피': 1, '코스닥': 1, '다우': 1, '한국': 1, '북한': 2, '축구': 3, '한동훈': 1, '베트남': 1, '월': 3, '모의고사': 1, '야구중계': 1, '빈소': 1, '공수처': 1, '이승환': 1, '후쿠오카': 1, '해병대': 1, '사령관': 1, '배추': 1, '고려고': 1, '정의선': 1, '함박도': 1, '세종시': 1, '초등학교': 1, '괴한': 1, '나경원': 1, '수소차': 1, '홍성흔': 1, '김정임': 1, '이사배': 1, '심은경': 1, '최연수': 1, '태연': 1, '더쇼': 1, '황하나': 1, '김동완': 1, '최준용': 1, '남북': 1, '무관중': 1, '가빈': 1, '황인범': 1, '전국': 1, '장애인': 1, '체육대회': 1, '기아': 1, '감독': 1, '이승엽': 1, '김상수': 1, '[정정보도문]': 1, '뉴스': 1, '시장의': 1, '고인': 1, '물': 1, '제휴평가위원회': 1, '관련': 2, '[정정보도]': 1, '[바코': 1, '인사이트]': 1, '경기도': 1, '농구협회': 1, '소속': 1, '김종부': 1, '심판': 1, '갑질': 1, '체험기': 1, '정정보도문': 1, '[바로잡습니다]': 2, '일자': 2, '면': 2, '코링크펀드에': 1, '억': 2, '출자한': 1, '회사': 1, '공정위': 1, '산하기관서': 1, '투자': 1, '받아': 1, '기사': 1, '중앙': 1, '사설': 1}\n",
      "정경심 --> 4\n",
      "조국 --> 10\n",
      "장관 --> 3\n",
      "서울대 --> 4\n",
      "복직 --> 3\n",
      "지하철 --> 4\n",
      "막판 --> 3\n",
      "설리 --> 3\n",
      "축구 --> 3\n",
      " >> 단어 전처리 결과 << \n",
      "{'정경심': 4, '조국': 10, '장관': 3, '서울대': 4, '복직': 3, '지하철': 4, '막판': 3, '설리': 3, '축구': 3}\n",
      ">>> top 5 <<<\n",
      "[('조국', 10), ('정경심', 4), ('서울대', 4), ('지하철', 4), ('장관', 3)]\n",
      "[('조국', 10), ('정경심', 4), ('서울대', 4), ('지하철', 4), ('장관', 3), ('복직', 3), ('막판', 3), ('설리', 3), ('축구', 3)]\n"
     ]
    }
   ],
   "source": [
    "### 단어 빈도수\n",
    "# 토픽 분석 : 특정 문서의 자료를 토대로 단어들의 출현 빈도수를 이용하여 해당 문서의 특징 및 경향을 분석하는 방법\n",
    "\n",
    "import pickle\n",
    "\n",
    "# pickle data file load\n",
    "file = open('data/data.pickle',mode='rb')       # with 없이 로드!\n",
    "news_data = pickle.load(file)\n",
    "\n",
    "# 텍스트 전처리 함수 정의\n",
    "import re\n",
    "def clean_text(text) :\n",
    "    # 문장부호 제거\n",
    "    text_re = re.sub(        # sub(변경할 패턴, 대체할 문자열, 변경할 자료)\n",
    "        '[,.?!:\\'\\\"]','',text      # 제거하면 기본적으로 공백으로 대체됨\n",
    "    )\n",
    "    # 특수문자, 숫자 제거\n",
    "    text_re= re.sub('[!@#$%^&*()<>=`~·]|[0-9]', '', text_re)\n",
    "    # 영문 소문자 제거\n",
    "    text_re = text_re.lower()\n",
    "    text_re = re.sub('[a-z]', '', text_re)\n",
    "    # 공백 제거\n",
    "    text_re = ' '.join(text_re.split()) # 공백 기준으로 각 단어를 분리하여 저장\n",
    "    return text_re\n",
    "\n",
    "clean_texts = [clean_text(row) for row in news_data]\n",
    "#print(' >>> 텍스트 전처리 결과 <<<')\n",
    "# print(clean_texts)\n",
    "\n",
    "# 단어 카운트\n",
    "word_count = {}\n",
    "\n",
    "for text in clean_texts :   # 텍스트 리스트 => 문장단위\n",
    "    for word in text.split() :   # 문장단위 => 단어\n",
    "        word_count[word] = word_count.get(word, 0) + 1      # key = '단어', value = 단어 개수\n",
    "print(' >> 단어 개수 카운트 <<')\n",
    "print(word_count)\n",
    "    \n",
    "# 단어 전처리\n",
    "# 불용 단어 제거\n",
    "del word_count['[바로잡습니다]']        # \"del\" target_list\n",
    "\n",
    "\n",
    "# 3회 이상 출력 단어 & 2~4자 단어 지정\n",
    "new_word_count = {}\n",
    "for word, cnt in word_count.items() :\n",
    "    if cnt >= 3 and len(word) >= 2 and len(word) <= 4 :\n",
    "        print(word, '-->', word_count[word])\n",
    "        new_word_count[word] = new_word_count.get(word, cnt)    # 조건에 맞는 값만 새로운 딕셔너리에 삽입\n",
    "        \n",
    "print(' >> 단어 전처리 결과 << ')\n",
    "print(new_word_count)\n",
    "\n",
    "# 빈도수 별 정렬 / 출력\n",
    "from collections import Counter     # 모듈 추가\n",
    "counter = Counter(new_word_count)\n",
    "top_5 = counter.most_common(5)      # 리스트 내에 튜플 형태로 출력\n",
    "ords = counter.most_common()\n",
    "print('>>> top 5 <<<')\n",
    "print(top_5)\n",
    "print(ords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "{'정부': 2, '마스크': 2, '출마': 3, '김동연': 2, '중대본': 2, '이준석': 2, '지방선거': 2, '민주당': 2}\n",
      "[('출마', 3), ('정부', 2), ('마스크', 2), ('김동연', 2), ('중대본', 2)]\n"
     ]
    }
   ],
   "source": [
    "# [실습] 직접 데이터 전처리, 빈도수 추출해보기\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "\n",
    "file = open('data/data.pickle1',mode='rb')       # 파일 읽어오기\n",
    "news = pickle.load(file)                         # 읽은 파일 데이터 변수에 넣기\n",
    "print(type(news))\n",
    "\n",
    "# 텍스트 전처리\n",
    "def cleaner(text) :\n",
    "    text_re = re.sub('[,.?!\\'\\\"`:;·]|[!@#$%^&*()-_=+/<>↑]|[0-9]|[a-z]', ' ', text)\n",
    "    text_re = ' '.join(text_re.split())\n",
    "    return text_re\n",
    "\n",
    "news_clean = [cleaner(row) for row in news]\n",
    "word_count = {}\n",
    "for line in news_clean :\n",
    "    for word in line.split() :\n",
    "        word_count[word] = word_count.get(word,0) + 1\n",
    "\n",
    "new_word_count = {}\n",
    "for word, cnt in word_count.items() :\n",
    "    if cnt >= 2 and len(word) > 1 :\n",
    "        new_word_count[word] = new_word_count.get(word, cnt)\n",
    "\n",
    "print(new_word_count)\n",
    "\n",
    "from collections import Counter\n",
    "counter = Counter(new_word_count)\n",
    "top_5 = counter.most_common(5)\n",
    "print(top_5)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b89b5cfaba6639976dc87ff2fec6d58faec662063367e2c229c520fe71072417"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
